{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVb9WF6h0dsn"
      },
      "source": [
        "\n",
        "\n",
        "Downloading two companion files where we have some useful function and also the main model architecture code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EIZ4Jd-FZo_",
        "outputId": "60b36381-a984-4482-afeb-7924ada33ff0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/autosoft-dev/ml-on-code/main/assets/model.py\n",
        "!wget https://raw.githubusercontent.com/autosoft-dev/ml-on-code/main/assets/utils.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-11 06:26:05--  https://raw.githubusercontent.com/autosoft-dev/ml-on-code/main/assets/model.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9935 (9.7K) [text/plain]\n",
            "Saving to: ‘model.py.1’\n",
            "\n",
            "\rmodel.py.1            0%[                    ]       0  --.-KB/s               \rmodel.py.1          100%[===================>]   9.70K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-11-11 06:26:05 (74.8 MB/s) - ‘model.py.1’ saved [9935/9935]\n",
            "\n",
            "--2022-11-11 06:26:05--  https://raw.githubusercontent.com/autosoft-dev/ml-on-code/main/assets/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2602 (2.5K) [text/plain]\n",
            "Saving to: ‘utils.py.1’\n",
            "\n",
            "utils.py.1          100%[===================>]   2.54K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-11-11 06:26:05 (33.5 MB/s) - ‘utils.py.1’ saved [2602/2602]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fSb9dkDGOtF"
      },
      "source": [
        "### installing tree-hugger and transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG-hxpQ6F-Zd",
        "outputId": "3c9e2f05-146b-4e92-e54d-28e5bee04d5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install -U tree-hugger PyYAML"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tree-hugger in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (6.0)\n",
            "Requirement already satisfied: pygit2 in /usr/local/lib/python3.7/dist-packages (from tree-hugger) (1.10.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from tree-hugger) (3.6.4)\n",
            "Requirement already satisfied: tree-sitter in /usr/local/lib/python3.7/dist-packages (from tree-hugger) (0.20.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from pygit2->tree-hugger) (1.5.2)\n",
            "Requirement already satisfied: cffi>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from pygit2->tree-hugger) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.9.1->pygit2->tree-hugger) (2.21)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tree-hugger) (1.11.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tree-hugger) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->tree-hugger) (57.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->tree-hugger) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tree-hugger) (1.4.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tree-hugger) (9.0.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tree-hugger) (22.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7I6O9LHZBtt"
      },
      "source": [
        "### Command to build the necessary processing libary (tree-hugger related)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bgf9OYFTBgZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a72883-5b18-4df0-b990-ad4c6689f7d8"
      },
      "source": [
        "!create_libs -c python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-11-11 07:47:49,509 INFO:Cloneing python repo from tree-sitter collections\n",
            "2022-11-11 07:48:15,375 INFO:Creating the library my-languages.so at /content\n",
            "2022-11-11 07:48:16,403 INFO:Finished creating library!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ4fxRRo1EJn"
      },
      "source": [
        "**Importing all necessary modules**\n",
        "\n",
        "and\n",
        "\n",
        "**Creating the Seq2Seq model for model architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90hu6R8cFyjd"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from utils import Example, convert_examples_to_features\n",
        "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer\n",
        "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    Build Seqence-to-Sequence.\n",
        "\n",
        "    Parameters:\n",
        "\n",
        "    * `encoder`- encoder of seq2seq model. e.g. roberta\n",
        "    * `decoder`- decoder of seq2seq model. e.g. transformer\n",
        "    * `config`- configuration of encoder model.\n",
        "    * `beam_size`- beam size for beam search.\n",
        "    * `max_length`- max length of target for beam search.\n",
        "    * `sos_id`- start of symbol ids in target for beam search.\n",
        "    * `eos_id`- end of symbol ids in target for beam search.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder,\n",
        "        decoder,\n",
        "        config,\n",
        "        beam_size=None,\n",
        "        max_length=None,\n",
        "        sos_id=None,\n",
        "        eos_id=None,\n",
        "    ):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.config = config\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(2048, 2048)))\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
        "        self.lsm = nn.LogSoftmax(dim=-1)\n",
        "        self.tie_weights()\n",
        "\n",
        "        self.beam_size = beam_size\n",
        "        self.max_length = max_length\n",
        "        self.sos_id = sos_id\n",
        "        self.eos_id = eos_id\n",
        "\n",
        "    def _tie_or_clone_weights(self, first_module, second_module):\n",
        "        \"\"\"Tie or clone module weights depending of weither we are using TorchScript or not\"\"\"\n",
        "        if self.config.torchscript:\n",
        "            first_module.weight = nn.Parameter(second_module.weight.clone())\n",
        "        else:\n",
        "            first_module.weight = second_module.weight\n",
        "\n",
        "    def tie_weights(self):\n",
        "        \"\"\"Make sure we are sharing the input and output embeddings.\n",
        "        Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n",
        "        \"\"\"\n",
        "        self._tie_or_clone_weights(\n",
        "            self.lm_head, self.encoder.embeddings.word_embeddings\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        source_ids=None,\n",
        "        source_mask=None,\n",
        "        target_ids=None,\n",
        "        target_mask=None,\n",
        "        args=None,\n",
        "    ):\n",
        "        outputs = self.encoder(source_ids, attention_mask=source_mask)\n",
        "        encoder_output = outputs[0].permute([1, 0, 2]).contiguous()\n",
        "        if target_ids is not None:\n",
        "            attn_mask = -1e4 * (\n",
        "                1 - self.bias[: target_ids.shape[1], : target_ids.shape[1]]\n",
        "            )\n",
        "            tgt_embeddings = (\n",
        "                self.encoder.embeddings(target_ids).permute([1, 0, 2]).contiguous()\n",
        "            )\n",
        "            out = self.decoder(\n",
        "                tgt_embeddings,\n",
        "                encoder_output,\n",
        "                tgt_mask=attn_mask,\n",
        "                memory_key_padding_mask=(1 - source_mask).bool(),\n",
        "            )\n",
        "            hidden_states = torch.tanh(self.dense(out)).permute([1, 0, 2]).contiguous()\n",
        "            lm_logits = self.lm_head(hidden_states)\n",
        "            # Shift so that tokens < n predict n\n",
        "            active_loss = target_mask[..., 1:].ne(0).view(-1) == 1\n",
        "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
        "            shift_labels = target_ids[..., 1:].contiguous()\n",
        "            # Flatten the tokens\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "            loss = loss_fct(\n",
        "                shift_logits.view(-1, shift_logits.size(-1))[active_loss],\n",
        "                shift_labels.view(-1)[active_loss],\n",
        "            )\n",
        "\n",
        "            outputs = loss, loss * active_loss.sum(), active_loss.sum()\n",
        "            return outputs\n",
        "        else:\n",
        "            # Predict\n",
        "            preds = []\n",
        "            if source_ids.device.type == \"cuda\":\n",
        "                zero = torch.cuda.LongTensor(1).fill_(0)\n",
        "            elif source_ids.device.type == \"cpu\":\n",
        "                zero = torch.LongTensor(1).fill_(0)\n",
        "            for i in range(source_ids.shape[0]):\n",
        "                context = encoder_output[:, i : i + 1]\n",
        "                context_mask = source_mask[i : i + 1, :]\n",
        "                beam = Beam(\n",
        "                    self.beam_size,\n",
        "                    self.sos_id,\n",
        "                    self.eos_id,\n",
        "                    device=source_ids.device.type,\n",
        "                )\n",
        "                input_ids = beam.getCurrentState()\n",
        "                context = context.repeat(1, self.beam_size, 1)\n",
        "                context_mask = context_mask.repeat(self.beam_size, 1)\n",
        "                for _ in range(self.max_length):\n",
        "                    if beam.done():\n",
        "                        break\n",
        "                    attn_mask = -1e4 * (\n",
        "                        1 - self.bias[: input_ids.shape[1], : input_ids.shape[1]]\n",
        "                    )\n",
        "                    tgt_embeddings = (\n",
        "                        self.encoder.embeddings(input_ids)\n",
        "                        .permute([1, 0, 2])\n",
        "                        .contiguous()\n",
        "                    )\n",
        "                    out = self.decoder(\n",
        "                        tgt_embeddings,\n",
        "                        context,\n",
        "                        tgt_mask=attn_mask,\n",
        "                        memory_key_padding_mask=(1 - context_mask).bool(),\n",
        "                    )\n",
        "                    out = torch.tanh(self.dense(out))\n",
        "                    hidden_states = out.permute([1, 0, 2]).contiguous()[:, -1, :]\n",
        "                    out = self.lsm(self.lm_head(hidden_states)).data\n",
        "                    beam.advance(out)\n",
        "                    input_ids.data.copy_(\n",
        "                        input_ids.data.index_select(0, beam.getCurrentOrigin())\n",
        "                    )\n",
        "                    input_ids = torch.cat((input_ids, beam.getCurrentState()), -1)\n",
        "                hyp = beam.getHyp(beam.getFinal())\n",
        "                pred = beam.buildTargetTokens(hyp)[: self.beam_size]\n",
        "                pred = [\n",
        "                    torch.cat(\n",
        "                        [x.view(-1) for x in p] + [zero] * (self.max_length - len(p))\n",
        "                    ).view(1, -1)\n",
        "                    for p in pred\n",
        "                ]\n",
        "                preds.append(torch.cat(pred, 0).unsqueeze(0))\n",
        "\n",
        "            preds = torch.cat(preds, 0)\n",
        "            return preds\n",
        "\n",
        "\n",
        "class Beam(object):\n",
        "    def __init__(self, size, sos, eos, device):\n",
        "        self.size = size\n",
        "        if device == \"cuda\":\n",
        "            self.tt = torch.cuda\n",
        "        elif device == \"cpu\":\n",
        "            self.tt = torch\n",
        "        # The score for each translation on the beam.\n",
        "        self.scores = self.tt.FloatTensor(size).zero_()\n",
        "        # The backpointers at each time-step.\n",
        "        self.prevKs = []\n",
        "        # The outputs at each time-step.\n",
        "        self.nextYs = [self.tt.LongTensor(size).fill_(0)]\n",
        "        self.nextYs[0][0] = sos\n",
        "        # Has EOS topped the beam yet.\n",
        "        self._eos = eos\n",
        "        self.eosTop = False\n",
        "        # Time and k pair for finished.\n",
        "        self.finished = []\n",
        "\n",
        "    def getCurrentState(self):\n",
        "        \"Get the outputs for the current timestep.\"\n",
        "        batch = self.tt.LongTensor(self.nextYs[-1]).view(-1, 1)\n",
        "        return batch\n",
        "\n",
        "    def getCurrentOrigin(self):\n",
        "        \"Get the backpointers for the current timestep.\"\n",
        "        return self.prevKs[-1]\n",
        "\n",
        "    def advance(self, wordLk):\n",
        "        \"\"\"\n",
        "        Given prob over words for every last beam `wordLk` and attention\n",
        "        `attnOut`: Compute and update the beam search.\n",
        "\n",
        "        Parameters:\n",
        "\n",
        "        * `wordLk`- probs of advancing from the last step (K x words)\n",
        "        * `attnOut`- attention at the last step\n",
        "\n",
        "        Returns: True if beam search is complete.\n",
        "        \"\"\"\n",
        "        numWords = wordLk.size(1)\n",
        "\n",
        "        # Sum the previous scores.\n",
        "        if len(self.prevKs) > 0:\n",
        "            beamLk = wordLk + self.scores.unsqueeze(1).expand_as(wordLk)\n",
        "\n",
        "            # Don't let EOS have children.\n",
        "            for i in range(self.nextYs[-1].size(0)):\n",
        "                if self.nextYs[-1][i] == self._eos:\n",
        "                    beamLk[i] = -1e20\n",
        "        else:\n",
        "            beamLk = wordLk[0]\n",
        "        flatBeamLk = beamLk.view(-1)\n",
        "        bestScores, bestScoresId = flatBeamLk.topk(self.size, 0, True, True)\n",
        "\n",
        "        self.scores = bestScores\n",
        "\n",
        "        # bestScoresId is flattened beam x word array, so calculate which\n",
        "        # word and beam each score came from\n",
        "        prevK = bestScoresId // numWords\n",
        "        self.prevKs.append(prevK)\n",
        "        self.nextYs.append((bestScoresId - prevK * numWords))\n",
        "\n",
        "        for i in range(self.nextYs[-1].size(0)):\n",
        "            if self.nextYs[-1][i] == self._eos:\n",
        "                s = self.scores[i]\n",
        "                self.finished.append((s, len(self.nextYs) - 1, i))\n",
        "\n",
        "        # End condition is when top-of-beam is EOS and no global score.\n",
        "        if self.nextYs[-1][0] == self._eos:\n",
        "            self.eosTop = True\n",
        "\n",
        "    def done(self):\n",
        "        return self.eosTop and len(self.finished) >= self.size\n",
        "\n",
        "    def getFinal(self):\n",
        "        if len(self.finished) == 0:\n",
        "            self.finished.append((self.scores[0], len(self.nextYs) - 1, 0))\n",
        "        self.finished.sort(key=lambda a: -a[0])\n",
        "        if len(self.finished) != self.size:\n",
        "            unfinished = []\n",
        "            for i in range(self.nextYs[-1].size(0)):\n",
        "                if self.nextYs[-1][i] != self._eos:\n",
        "                    s = self.scores[i]\n",
        "                    unfinished.append((s, len(self.nextYs) - 1, i))\n",
        "            unfinished.sort(key=lambda a: -a[0])\n",
        "            self.finished += unfinished[: self.size - len(self.finished)]\n",
        "        return self.finished[: self.size]\n",
        "\n",
        "    def getHyp(self, beam_res):\n",
        "        \"\"\"\n",
        "        Walk back to construct the full hypothesis.\n",
        "        \"\"\"\n",
        "        hyps = []\n",
        "        for _, timestep, k in beam_res:\n",
        "            hyp = []\n",
        "            for j in range(len(self.prevKs[:timestep]) - 1, -1, -1):\n",
        "                hyp.append(self.nextYs[j + 1][k])\n",
        "                k = self.prevKs[j][k]\n",
        "            hyps.append(hyp[::-1])\n",
        "        return hyps\n",
        "\n",
        "    def buildTargetTokens(self, preds):\n",
        "        sentence = []\n",
        "        for pred in preds:\n",
        "            tokens = []\n",
        "            for tok in pred:\n",
        "                if tok == self._eos:\n",
        "                    break\n",
        "                tokens.append(tok)\n",
        "            sentence.append(tokens)\n",
        "        return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDqbzy7L1Qac"
      },
      "source": [
        "The simple model was not having a good score. So we decided to import a fine tuned model for this purpose which was trained on the similar kind of dataset. Importing the same from the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCSFPT6xYtkA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42615cb9-5d7e-4eee-98b3-2c296395aa11"
      },
      "source": [
        "!wget https://code-summary.s3.amazonaws.com/pytorch_model.bin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-11 07:48:52--  https://code-summary.s3.amazonaws.com/pytorch_model.bin\n",
            "Resolving code-summary.s3.amazonaws.com (code-summary.s3.amazonaws.com)... 52.216.97.3\n",
            "Connecting to code-summary.s3.amazonaws.com (code-summary.s3.amazonaws.com)|52.216.97.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 706871064 (674M) [application/macbinary]\n",
            "Saving to: ‘pytorch_model.bin.2’\n",
            "\n",
            "pytorch_model.bin.2 100%[===================>] 674.12M  47.2MB/s    in 13s     \n",
            "\n",
            "2022-11-11 07:49:06 (51.9 MB/s) - ‘pytorch_model.bin.2’ saved [706871064/706871064]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC8ol4ii2JOq"
      },
      "source": [
        "## We are defining all the needed functions here. \n",
        "def inference(data, model, tokenizer):\n",
        "    # Calculate bleu\n",
        "    eval_sampler = SequentialSampler(data)\n",
        "    eval_dataloader = DataLoader(data, sampler=eval_sampler, batch_size=len(data))\n",
        "\n",
        "    model.eval()\n",
        "    p = []\n",
        "    for batch in eval_dataloader:\n",
        "        batch = tuple(t.to('cpu') for t in batch)\n",
        "        source_ids, source_mask = batch\n",
        "        with torch.no_grad():\n",
        "            preds = model(source_ids=source_ids, source_mask=source_mask)\n",
        "            for pred in preds:\n",
        "                t = pred[0].cpu().numpy()\n",
        "                t = list(t)\n",
        "                if 0 in t:\n",
        "                    t = t[: t.index(0)]\n",
        "                text = tokenizer.decode(t, clean_up_tokenization_spaces=False)\n",
        "                p.append(text)\n",
        "    return (p, source_ids.shape[-1])\n",
        "\n",
        "\n",
        "def get_features(examples, tokenizer):\n",
        "    features = convert_examples_to_features(\n",
        "        examples, tokenizer, stage=\"test\"\n",
        "    )\n",
        "    all_source_ids = torch.tensor(\n",
        "        [f.source_ids[: 256] for f in features], dtype=torch.long\n",
        "    )\n",
        "    all_source_mask = torch.tensor(\n",
        "        [f.source_mask[: 256] for f in features], dtype=torch.long\n",
        "    )\n",
        "    return TensorDataset(all_source_ids, all_source_mask)\n",
        "\n",
        "\n",
        "def build_model(model_class, config, tokenizer):\n",
        "    encoder = model_class(config=config)\n",
        "    decoder_layer = nn.TransformerDecoderLayer(\n",
        "        d_model=config.hidden_size, nhead=config.num_attention_heads\n",
        "    )\n",
        "    decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
        "    model = Seq2Seq(\n",
        "        encoder=encoder,\n",
        "        decoder=decoder,\n",
        "        config=config,\n",
        "        beam_size=10,\n",
        "        max_length=128,\n",
        "        sos_id=tokenizer.cls_token_id,\n",
        "        eos_id=tokenizer.sep_token_id,\n",
        "    )\n",
        "\n",
        "    model.load_state_dict(\n",
        "        torch.load(\n",
        "            \"pytorch_model.bin\",\n",
        "            map_location=torch.device(\"cpu\"),\n",
        "        ),\n",
        "        strict=False,\n",
        "    )\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LJ-LOTJ2TzB"
      },
      "source": [
        "Now that we have all the needed functions,loading the baseline model from Hugging Face model hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWFeZDn52a0H"
      },
      "source": [
        "config = RobertaConfig.from_pretrained(\"microsoft/codebert-base\")\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\n",
        "    \"microsoft/codebert-base\", do_lower_case=False\n",
        ")\n",
        "\n",
        "model = build_model(\n",
        "    model_class=RobertaModel, config=config, tokenizer=tokenizer\n",
        ").to('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp9j7zSa2iX-"
      },
      "source": [
        "Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cBV950F20bc",
        "outputId": "f9c6010e-64a7-4620-9d63-f55e240517b1"
      },
      "source": [
        "example = [Example(source=\"def fn_tensors(t, t1) -> Any:\\n    return t + t1\", target=None)]\n",
        "message, length = inference(get_features(example, tokenizer), model, tokenizer)\n",
        "print(message)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:220: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Compute the difference between two tensors .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFBsTi-A27ZH"
      },
      "source": [
        "\n",
        "We need to be able to run it on a bunch of files and extract the functions from it and then predict their docstrings. How shall we do it?\n",
        "\n",
        "Codist [tree-hugger](https://github.com/autosoft-dev/tree-hugger) to the rescue!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHM0NAnCeLaf"
      },
      "source": [
        "We are going to declare a small function that will help us go over each files in a nested directory tree (like the one above we cloned) and get each file at a time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cWsYhNyeIb5"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def check_out_path(target_path: Path):\n",
        "    \"\"\"\"\n",
        "    This function recursively yields all contents of a pathlib.Path object\n",
        "    \"\"\"\n",
        "    yield target_path\n",
        "    for file in target_path.iterdir():\n",
        "        if file.is_dir():\n",
        "            yield from check_out_path(file)\n",
        "        else:\n",
        "            yield file.absolute()\n",
        "\n",
        "\n",
        "def is_python_file(file_path: Path):\n",
        "  \"\"\"\n",
        "  This little function will help us to filter the result and keep only the python files\n",
        "  \"\"\"\n",
        "  return file_path.is_file() and file_path.suffix == \".py\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB6OniXlenyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7f1ee72-6cc7-40ac-9f6f-49270e46adb4"
      },
      "source": [
        "for file_path in check_out_path(Path(\"py-files\")):\n",
        "  if is_python_file(file_path):\n",
        "    print(file_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/py-files/api.py\n",
            "/content/py-files/mlapi.py\n",
            "/content/py-files/main.py\n",
            "/content/py-files/mainapp.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxp1hwkWgKOQ"
      },
      "source": [
        "Using tree-hugger to parse all the needed files and let's do that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTQTIPvFexFo"
      },
      "source": [
        "# We first create our PythonParser object\n",
        "from tree_hugger.core import PythonParser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrlIUSCBgecv"
      },
      "source": [
        "pp = PythonParser(library_loc=\"/content/my-languages.so\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8Eyii31gjk1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3f3555a-bb92-4802-9597-9e28653c5e10"
      },
      "source": [
        "# Let's use the function we defined before to go over all the files.\n",
        "for file_path in check_out_path(Path(\"py-files\")):\n",
        "  if is_python_file(file_path):\n",
        "    # we use one line, super convinient tree-hugger API call to get the needed data\n",
        "    if pp.parse_file(str(file_path)):\n",
        "      temp_cache = []\n",
        "      # The following call returns a dict where each key is a name of a function\n",
        "      # And each value is a tuple, (function_body, function_docstring)\n",
        "      func_and_docstr = pp.get_all_function_bodies(strip_docstr=True)\n",
        "      for func_name, (body, docstr) in func_and_docstr.items():\n",
        "        example = [Example(source=body, target=None)]\n",
        "        message, length = inference(get_features(example, tokenizer), model, tokenizer)\n",
        "        print(\"Function name:\",func_name, \", \\t\",\"Function definition/Work :\",\" \".join(message))\n",
        "      # Let's add the result to the final output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:220: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function name: scoring_endpoint , \t Function definition/Work : estimate scoring endpoint\n",
            "Function name: engine_talk , \t Function definition/Work : send text to engine\n",
            "Function name: user_commands , \t Function definition/Work : Extract user commands .\n",
            "Function name: run_blanco , \t Function definition/Work : Run the bot .\n",
            "Function name: index , \t Function definition/Work : Return the index .\n",
            "Function name: stomach_disesase_pred , \t Function definition/Work : Evaluate gadgets .\n",
            "Function name: hepatitis_disease_pred , \t Function definition/Work : Evaluate hepatitis disease prediction .\n",
            "Function name: skin_disease_pred , \t Function definition/Work : Evaluate the skin disease prediction .\n",
            "Function name: medical_insurance_price_pred , \t Function definition/Work : Compute medicalurance price prediction .\n",
            "Function name: breast_cancer_pred , \t Function definition/Work : This function is used to make sure that there is no cancer cases .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H8J889PCKkiv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}